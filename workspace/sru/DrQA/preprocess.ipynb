{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2017 08:22:14 start data preparing...\n",
      "11/09/2017 08:22:52 glove loaded.\n",
      "11/09/2017 08:22:54 json data flattened.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "973708545035325856",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f0d20a116207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# exit(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m context_tokens = [[w.text for w in doc] for doc in nlp.pipe(\n\u001b[0;32m--> 137\u001b[0;31m     context_iter, batch_size=args.batch_size, n_threads=args.threads)]\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'got intial tokens.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f0d20a116207>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# print(c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# exit(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m context_tokens = [[w.text for w in doc] for doc in nlp.pipe(\n\u001b[0m\u001b[1;32m    137\u001b[0m     context_iter, batch_size=args.batch_size, n_threads=args.threads)]\n\u001b[1;32m    138\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'got intial tokens.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-f0d20a116207>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# print(c)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# exit(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m context_tokens = [[w.text for w in doc] for doc in nlp.pipe(\n\u001b[0m\u001b[1;32m    137\u001b[0m     context_iter, batch_size=args.batch_size, n_threads=args.threads)]\n\u001b[1;32m    138\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'got intial tokens.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtoken.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.text.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtoken.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.orth_.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mstrings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 973708545035325856"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import msgpack\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import collections\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import logging\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Preprocessing data files, about 10 minitues to run.'\n",
    ")\n",
    "parser.add_argument('--wv_file', default='glove/glove.840B.300d.txt',\n",
    "                    help='path to word vector file.')\n",
    "parser.add_argument('--wv_dim', type=int, default=300,\n",
    "                    help='word vector dimension.')\n",
    "parser.add_argument('--wv_cased', type=bool, default=True,\n",
    "                    help='treat the words as cased or not.')\n",
    "parser.add_argument('--sort_all', action='store_true',\n",
    "                    help='sort the vocabulary by frequencies of all words. '\n",
    "                         'Otherwise consider question words first.')\n",
    "parser.add_argument('--sample_size', type=int, default=0,\n",
    "                    help='size of sample data (for debugging).')\n",
    "parser.add_argument('--threads', type=int, default=multiprocessing.cpu_count(),\n",
    "                    help='number of threads for preprocessing.')\n",
    "parser.add_argument('--batch_size', type=int, default=64,\n",
    "                    help='batch size for multiprocess tokenizing and tagging.')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "trn_file = 'SQuAD/train-v1.1.json'\n",
    "dev_file = 'SQuAD/dev-v1.1.json'\n",
    "wv_file = args.wv_file\n",
    "wv_dim = args.wv_dim\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.DEBUG,\n",
    "                    datefmt='%m/%d/%Y %I:%M:%S')\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "log.info('start data preparing...')\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "\n",
    "def load_wv_vocab(file):\n",
    "    '''Load tokens from word vector file.\n",
    "\n",
    "    Only tokens are loaded. Vectors are not loaded at this time for space efficiency.\n",
    "\n",
    "    Args:\n",
    "        file (str): path of pretrained word vector file.\n",
    "\n",
    "    Returns:\n",
    "        set: a set of tokens (str) contained in the word vector file.\n",
    "    '''\n",
    "    vocab = set()\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            elems = line.split()\n",
    "            token = normalize_text(''.join(elems[0:-wv_dim]))  # a token may contain space\n",
    "            vocab.add(token)\n",
    "    return vocab\n",
    "wv_vocab = load_wv_vocab(wv_file)\n",
    "log.info('glove loaded.')\n",
    "\n",
    "\n",
    "def flatten_json(file, proc_func):\n",
    "    '''A multi-processing wrapper for loading SQuAD data file.'''\n",
    "    with open(file) as f:\n",
    "        raw_json = json.load(f)\n",
    "        if 'data' in raw_json:\n",
    "            data = raw_json['data']\n",
    "        else:\n",
    "            data = raw_json\n",
    "    with ProcessPoolExecutor(max_workers=args.threads) as executor:\n",
    "        rows = executor.map(proc_func, data)\n",
    "    rows = sum(rows, [])\n",
    "    return rows\n",
    "\n",
    "\n",
    "def proc_train(article):\n",
    "    '''Flatten each article in training data.'''\n",
    "    rows = []\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            id_, question= qa['id'], qa['question']\n",
    "            if 'answers' in qa:\n",
    "                answers = qa['answers']\n",
    "            else:\n",
    "                answers=[]\n",
    "                answers.append(qa['answer'])\n",
    "            answer = answers[0]['text']  # in training data there's only one answer\n",
    "            answer_start = answers[0]['answer_start']\n",
    "            answer_end = answer_start + len(answer)\n",
    "            rows.append((id_, context, question, answer, answer_start, answer_end))\n",
    "    return rows\n",
    "\n",
    "\n",
    "def proc_dev(article):\n",
    "    '''Flatten each article in dev data'''\n",
    "    rows = []\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            id_, question, answers = qa['id'], qa['question'], qa['answers']\n",
    "            answers = [a['text'] for a in answers]\n",
    "            rows.append((id_, context, question, answers))\n",
    "    return rows\n",
    "train = flatten_json(trn_file, proc_train)\n",
    "train = pd.DataFrame(train,\n",
    "                     columns=['id', 'context', 'question', 'answer',\n",
    "                              'answer_start', 'answer_end'])\n",
    "dev = flatten_json(dev_file, proc_dev)\n",
    "dev = pd.DataFrame(dev,\n",
    "                   columns=['id', 'context', 'question', 'answers'])\n",
    "log.info('json data flattened.')\n",
    "#exit(0)\n",
    "\n",
    "nlp = spacy.load('en', parser=False, tagger=False, entity=False)\n",
    "\n",
    "\n",
    "def pre_proc(text):\n",
    "    '''normalize spaces in a string.'''\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "context_iter = (pre_proc(c) for c in train.context)\n",
    "# for c in context_iter:\n",
    "    # print(c)\n",
    "    # exit(0)\n",
    "context_tokens = [[w.text for w in doc] for doc in nlp.pipe(\n",
    "    context_iter, batch_size=args.batch_size, n_threads=args.threads)]\n",
    "log.info('got intial tokens.')\n",
    "\n",
    "\n",
    "def get_answer_index(context, context_token, answer_start, answer_end):\n",
    "    '''\n",
    "    Get exact indices of the answer in the tokens of the passage,\n",
    "    according to the start and end position of the answer.\n",
    "\n",
    "    Args:\n",
    "        context (str): the context passage\n",
    "        context_token (list): list of tokens (str) in the context passage\n",
    "        answer_start (int): the start position of the answer in the passage\n",
    "        answer_end (int): the end position of the answer in the passage\n",
    "\n",
    "    Returns:\n",
    "        (int, int): start index and end index of answer\n",
    "    '''\n",
    "    p_str = 0\n",
    "    p_token = 0\n",
    "    while p_str < len(context):\n",
    "        if re.match('\\s', context[p_str]):\n",
    "            p_str += 1\n",
    "            continue\n",
    "        token = context_token[p_token]\n",
    "        token_len = len(token)\n",
    "        if context[p_str:p_str + token_len] != token:\n",
    "            return (None, None)\n",
    "        if p_str == answer_start:\n",
    "            t_start = p_token\n",
    "        p_str += token_len\n",
    "        if p_str == answer_end:\n",
    "            try:\n",
    "                return (t_start, p_token)\n",
    "            except UnboundLocalError as e:\n",
    "                return (None, None)\n",
    "        p_token += 1\n",
    "    return (None, None)\n",
    "train['answer_start_token'], train['answer_end_token'] = \\\n",
    "    zip(*[get_answer_index(a, b, c, d) for a, b, c, d in\n",
    "          zip(train.context, context_tokens,\n",
    "              train.answer_start, train.answer_end)])\n",
    "initial_len = len(train)\n",
    "train.dropna(inplace=True)\n",
    "log.info('drop {} inconsistent samples.'.format(initial_len - len(train)))\n",
    "log.info('answer pointer generated.')\n",
    "\n",
    "questions = list(train.question) + list(dev.question)\n",
    "contexts = list(train.context) + list(dev.context)\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "context_text = [pre_proc(c) for c in contexts]\n",
    "question_text = [pre_proc(q) for q in questions]\n",
    "question_docs = [doc for doc in nlp.pipe(\n",
    "    iter(question_text), batch_size=args.batch_size, n_threads=args.threads)]\n",
    "context_docs = [doc for doc in nlp.pipe(\n",
    "    iter(context_text), batch_size=args.batch_size, n_threads=args.threads)]\n",
    "if args.wv_cased:\n",
    "    question_tokens = [[normalize_text(w.text) for w in doc] for doc in question_docs]\n",
    "    context_tokens = [[normalize_text(w.text) for w in doc] for doc in context_docs]\n",
    "else:\n",
    "    question_tokens = [[normalize_text(w.text).lower() for w in doc] for doc in question_docs]\n",
    "    context_tokens = [[normalize_text(w.text).lower() for w in doc] for doc in context_docs]\n",
    "context_token_span = [[(w.idx, w.idx + len(w.text)) for w in doc] for doc in context_docs]\n",
    "context_tags = [[w.tag_ for w in doc] for doc in context_docs]\n",
    "context_ents = [[w.ent_type_ for w in doc] for doc in context_docs]\n",
    "context_features = []\n",
    "for question, context in zip(question_docs, context_docs):\n",
    "    question_word = {w.text for w in question}\n",
    "    question_lower = {w.text.lower() for w in question}\n",
    "    question_lemma = {w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in question}\n",
    "    match_origin = [w.text in question_word for w in context]\n",
    "    match_lower = [w.text.lower() in question_lower for w in context]\n",
    "    match_lemma = [(w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower()) in question_lemma for w in context]\n",
    "    context_features.append(list(zip(match_origin, match_lower, match_lemma)))\n",
    "log.info('tokens generated')\n",
    "\n",
    "\n",
    "def build_vocab(questions, contexts):\n",
    "    '''\n",
    "    Build vocabulary sorted by global word frequency, or consider frequencies in questions first,\n",
    "    which is controlled by `args.sort_all`.\n",
    "    '''\n",
    "    if args.sort_all:\n",
    "        counter = collections.Counter(w for doc in questions + contexts for w in doc)\n",
    "        vocab = sorted([t for t in counter if t in wv_vocab], key=counter.get, reverse=True)\n",
    "    else:\n",
    "        counter_q = collections.Counter(w for doc in questions for w in doc)\n",
    "        counter_c = collections.Counter(w for doc in contexts for w in doc)\n",
    "        counter = counter_c + counter_q\n",
    "        vocab = sorted([t for t in counter_q if t in wv_vocab], key=counter_q.get, reverse=True)\n",
    "        vocab += sorted([t for t in counter_c.keys() - counter_q.keys() if t in wv_vocab],\n",
    "                        key=counter.get, reverse=True)\n",
    "    total = sum(counter.values())\n",
    "    matched = sum(counter[t] for t in vocab)\n",
    "    log.info('vocab coverage {1}/{0} | OOV occurrence {2}/{3} ({4:.4f}%)'.format(\n",
    "        len(counter), len(vocab), (total - matched), total, (total - matched) / total * 100))\n",
    "    vocab.insert(0, \"<PAD>\")\n",
    "    vocab.insert(1, \"<UNK>\")\n",
    "    return vocab, counter\n",
    "\n",
    "\n",
    "def token2id(docs, vocab, unk_id=None):\n",
    "    w2id = {w: i for i, w in enumerate(vocab)}\n",
    "    ids = [[w2id[w] if w in w2id else unk_id for w in doc] for doc in docs]\n",
    "    return ids\n",
    "vocab, counter = build_vocab(question_tokens, context_tokens)\n",
    "# tokens\n",
    "question_ids = token2id(question_tokens, vocab, unk_id=1)\n",
    "context_ids = token2id(context_tokens, vocab, unk_id=1)\n",
    "# term frequency in document\n",
    "context_tf = []\n",
    "for doc in context_tokens:\n",
    "    counter_ = collections.Counter(w.lower() for w in doc)\n",
    "    total = sum(counter_.values())\n",
    "    context_tf.append([counter_[w.lower()] / total for w in doc])\n",
    "context_features = [[list(w) + [tf] for w, tf in zip(doc, tfs)] for doc, tfs in\n",
    "                    zip(context_features, context_tf)]\n",
    "# tags\n",
    "vocab_tag = list(nlp.tagger.tag_names)\n",
    "context_tag_ids = token2id(context_tags, vocab_tag)\n",
    "# entities, build dict on the fly\n",
    "counter_ent = collections.Counter(w for doc in context_ents for w in doc)\n",
    "vocab_ent = sorted(counter_ent, key=counter_ent.get, reverse=True)\n",
    "log.info('Found {} POS tags.'.format(len(vocab_tag)))\n",
    "log.info('Found {} entity tags: {}'.format(len(vocab_ent), vocab_ent))\n",
    "context_ent_ids = token2id(context_ents, vocab_ent)\n",
    "log.info('vocab built.')\n",
    "\n",
    "\n",
    "def build_embedding(embed_file, targ_vocab, dim_vec):\n",
    "    vocab_size = len(targ_vocab)\n",
    "    emb = np.zeros((vocab_size, dim_vec))\n",
    "    w2id = {w: i for i, w in enumerate(targ_vocab)}\n",
    "    with open(embed_file) as f:\n",
    "        for line in f:\n",
    "            elems = line.split()\n",
    "            token = normalize_text(''.join(elems[0:-wv_dim]))\n",
    "            if token in w2id:\n",
    "                emb[w2id[token]] = [float(v) for v in elems[-wv_dim:]]\n",
    "    return emb\n",
    "embedding = build_embedding(wv_file, vocab, wv_dim)\n",
    "log.info('got embedding matrix.')\n",
    "\n",
    "train.to_csv('SQuAD/train.csv', index=False)\n",
    "dev.to_csv('SQuAD/dev.csv', index=False)\n",
    "meta = {\n",
    "    'vocab': vocab,\n",
    "    'embedding': embedding.tolist()\n",
    "}\n",
    "with open('SQuAD/meta.msgpack', 'wb') as f:\n",
    "    msgpack.dump(meta, f)\n",
    "result = {\n",
    "    'trn_question_ids': question_ids[:len(train)],\n",
    "    'dev_question_ids': question_ids[len(train):],\n",
    "    'trn_context_ids': context_ids[:len(train)],\n",
    "    'dev_context_ids': context_ids[len(train):],\n",
    "    'trn_context_features': context_features[:len(train)],\n",
    "    'dev_context_features': context_features[len(train):],\n",
    "    'trn_context_tags': context_tag_ids[:len(train)],\n",
    "    'dev_context_tags': context_tag_ids[len(train):],\n",
    "    'trn_context_ents': context_ent_ids[:len(train)],\n",
    "    'dev_context_ents': context_ent_ids[len(train):],\n",
    "    'trn_context_text': context_text[:len(train)],\n",
    "    'dev_context_text': context_text[len(train):],\n",
    "    'trn_context_spans': context_token_span[:len(train)],\n",
    "    'dev_context_spans': context_token_span[len(train):]\n",
    "}\n",
    "with open('SQuAD/data.msgpack', 'wb') as f:\n",
    "    msgpack.dump(result, f)\n",
    "if args.sample_size:\n",
    "    sample_size = args.sample_size\n",
    "    sample = {\n",
    "        'trn_question_ids': result['trn_question_ids'][:sample_size],\n",
    "        'dev_question_ids': result['dev_question_ids'][:sample_size],\n",
    "        'trn_context_ids': result['trn_context_ids'][:sample_size],\n",
    "        'dev_context_ids': result['dev_context_ids'][:sample_size],\n",
    "        'trn_context_features': result['trn_context_features'][:sample_size],\n",
    "        'dev_context_features': result['dev_context_features'][:sample_size],\n",
    "        'trn_context_tags': result['trn_context_tags'][:sample_size],\n",
    "        'dev_context_tags': result['dev_context_tags'][:sample_size],\n",
    "        'trn_context_ents': result['trn_context_ents'][:sample_size],\n",
    "        'dev_context_ents': result['dev_context_ents'][:sample_size],\n",
    "        'trn_context_text': result['trn_context_text'][:sample_size],\n",
    "        'dev_context_text': result['dev_context_text'][:sample_size],\n",
    "        'trn_context_spans': result['trn_context_spans'][:sample_size],\n",
    "        'dev_context_spans': result['dev_context_spans'][:sample_size]\n",
    "    }\n",
    "    with open('SQuAD/sample.msgpack', 'wb') as f:\n",
    "        msgpack.dump(sample, f)\n",
    "log.info('saved to disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2017 09:35:38 got intial tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pre_proc(text):\n",
    "    '''normalize spaces in a string.'''\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "context_iter = [pre_proc(c) for c in train.context]\n",
    "# for c in context_iter:\n",
    "    # print(c)\n",
    "    # exit(0)\n",
    "    \n",
    "context_tokens = [[w.text for w in doc] for doc in nlp.pipe(\n",
    "    context_iter, batch_size=args.batch_size, n_threads=args.threads)]\n",
    "log.info('got intial tokens.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/09/2017 11:02:42 drop 776 inconsistent samples.\n",
      "11/09/2017 11:02:42 answer pointer generated.\n",
      "11/10/2017 12:03:30 tokens generated\n",
      "11/10/2017 12:03:33 vocab coverage 91541/111048 | OOV occurrence 109677/14810860 (0.7405%)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tagger' object has no attribute 'tag_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-90ab5de7c573>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m                     zip(context_features, context_tf)]\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mvocab_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0mcontext_tag_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# entities, build dict on the fly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tagger' object has no attribute 'tag_names'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_answer_index(context, context_token, answer_start, answer_end):\n",
    "    '''\n",
    "    Get exact indices of the answer in the tokens of the passage,\n",
    "    according to the start and end position of the answer.\n",
    "\n",
    "    Args:\n",
    "        context (str): the context passage\n",
    "        context_token (list): list of tokens (str) in the context passage\n",
    "        answer_start (int): the start position of the answer in the passage\n",
    "        answer_end (int): the end position of the answer in the passage\n",
    "\n",
    "    Returns:\n",
    "        (int, int): start index and end index of answer\n",
    "    '''\n",
    "    p_str = 0\n",
    "    p_token = 0\n",
    "    while p_str < len(context):\n",
    "        if re.match('\\s', context[p_str]):\n",
    "            p_str += 1\n",
    "            continue\n",
    "        token = context_token[p_token]\n",
    "        token_len = len(token)\n",
    "        if context[p_str:p_str + token_len] != token:\n",
    "            return (None, None)\n",
    "        if p_str == answer_start:\n",
    "            t_start = p_token\n",
    "        p_str += token_len\n",
    "        if p_str == answer_end:\n",
    "            try:\n",
    "                return (t_start, p_token)\n",
    "            except UnboundLocalError as e:\n",
    "                return (None, None)\n",
    "        p_token += 1\n",
    "    return (None, None)\n",
    "train['answer_start_token'], train['answer_end_token'] = \\\n",
    "    zip(*[get_answer_index(a, b, c, d) for a, b, c, d in\n",
    "          zip(train.context, context_tokens,\n",
    "              train.answer_start, train.answer_end)])\n",
    "initial_len = len(train)\n",
    "train.dropna(inplace=True)\n",
    "log.info('drop {} inconsistent samples.'.format(initial_len - len(train)))\n",
    "log.info('answer pointer generated.')\n",
    "\n",
    "questions = list(train.question) + list(dev.question)\n",
    "contexts = list(train.context) + list(dev.context)\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "context_text = [pre_proc(c) for c in contexts]\n",
    "question_text = [pre_proc(q) for q in questions]\n",
    "question_docs = [doc for doc in nlp.pipe(\n",
    "    iter(question_text), batch_size=args.batch_size, n_threads=args.threads)]\n",
    "context_docs = [doc for doc in nlp.pipe(\n",
    "    iter(context_text), batch_size=args.batch_size, n_threads=args.threads)]\n",
    "if args.wv_cased:\n",
    "    question_tokens = [[normalize_text(w.text) for w in doc] for doc in question_docs]\n",
    "    context_tokens = [[normalize_text(w.text) for w in doc] for doc in context_docs]\n",
    "else:\n",
    "    question_tokens = [[normalize_text(w.text).lower() for w in doc] for doc in question_docs]\n",
    "    context_tokens = [[normalize_text(w.text).lower() for w in doc] for doc in context_docs]\n",
    "context_token_span = [[(w.idx, w.idx + len(w.text)) for w in doc] for doc in context_docs]\n",
    "context_tags = [[w.tag_ for w in doc] for doc in context_docs]\n",
    "context_ents = [[w.ent_type_ for w in doc] for doc in context_docs]\n",
    "context_features = []\n",
    "for question, context in zip(question_docs, context_docs):\n",
    "    question_word = {w.text for w in question}\n",
    "    question_lower = {w.text.lower() for w in question}\n",
    "    question_lemma = {w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower() for w in question}\n",
    "    match_origin = [w.text in question_word for w in context]\n",
    "    match_lower = [w.text.lower() in question_lower for w in context]\n",
    "    match_lemma = [(w.lemma_ if w.lemma_ != '-PRON-' else w.text.lower()) in question_lemma for w in context]\n",
    "    context_features.append(list(zip(match_origin, match_lower, match_lemma)))\n",
    "log.info('tokens generated')\n",
    "\n",
    "\n",
    "def build_vocab(questions, contexts):\n",
    "    '''\n",
    "    Build vocabulary sorted by global word frequency, or consider frequencies in questions first,\n",
    "    which is controlled by `args.sort_all`.\n",
    "    '''\n",
    "    if args.sort_all:\n",
    "        counter = collections.Counter(w for doc in questions + contexts for w in doc)\n",
    "        vocab = sorted([t for t in counter if t in wv_vocab], key=counter.get, reverse=True)\n",
    "    else:\n",
    "        counter_q = collections.Counter(w for doc in questions for w in doc)\n",
    "        counter_c = collections.Counter(w for doc in contexts for w in doc)\n",
    "        counter = counter_c + counter_q\n",
    "        vocab = sorted([t for t in counter_q if t in wv_vocab], key=counter_q.get, reverse=True)\n",
    "        vocab += sorted([t for t in counter_c.keys() - counter_q.keys() if t in wv_vocab],\n",
    "                        key=counter.get, reverse=True)\n",
    "    total = sum(counter.values())\n",
    "    matched = sum(counter[t] for t in vocab)\n",
    "    log.info('vocab coverage {1}/{0} | OOV occurrence {2}/{3} ({4:.4f}%)'.format(\n",
    "        len(counter), len(vocab), (total - matched), total, (total - matched) / total * 100))\n",
    "    vocab.insert(0, \"<PAD>\")\n",
    "    vocab.insert(1, \"<UNK>\")\n",
    "    return vocab, counter\n",
    "\n",
    "\n",
    "def token2id(docs, vocab, unk_id=None):\n",
    "    w2id = {w: i for i, w in enumerate(vocab)}\n",
    "    ids = [[w2id[w] if w in w2id else unk_id for w in doc] for doc in docs]\n",
    "    return ids\n",
    "vocab, counter = build_vocab(question_tokens, context_tokens)\n",
    "# tokens\n",
    "question_ids = token2id(question_tokens, vocab, unk_id=1)\n",
    "context_ids = token2id(context_tokens, vocab, unk_id=1)\n",
    "# term frequency in document\n",
    "context_tf = []\n",
    "for doc in context_tokens:\n",
    "    counter_ = collections.Counter(w.lower() for w in doc)\n",
    "    total = sum(counter_.values())\n",
    "    context_tf.append([counter_[w.lower()] / total for w in doc])\n",
    "context_features = [[list(w) + [tf] for w, tf in zip(doc, tfs)] for doc, tfs in\n",
    "                    zip(context_features, context_tf)]\n",
    "# tags\n",
    "vocab_tag = list(nlp.tagger.tag_names)\n",
    "context_tag_ids = token2id(context_tags, vocab_tag)\n",
    "# entities, build dict on the fly\n",
    "counter_ent = collections.Counter(w for doc in context_ents for w in doc)\n",
    "vocab_ent = sorted(counter_ent, key=counter_ent.get, reverse=True)\n",
    "log.info('Found {} POS tags.'.format(len(vocab_tag)))\n",
    "log.info('Found {} entity tags: {}'.format(len(vocab_ent), vocab_ent))\n",
    "context_ent_ids = token2id(context_ents, vocab_ent)\n",
    "log.info('vocab built.')\n",
    "\n",
    "\n",
    "def build_embedding(embed_file, targ_vocab, dim_vec):\n",
    "    vocab_size = len(targ_vocab)\n",
    "    emb = np.zeros((vocab_size, dim_vec))\n",
    "    w2id = {w: i for i, w in enumerate(targ_vocab)}\n",
    "    with open(embed_file) as f:\n",
    "        for line in f:\n",
    "            elems = line.split()\n",
    "            token = normalize_text(''.join(elems[0:-wv_dim]))\n",
    "            if token in w2id:\n",
    "                emb[w2id[token]] = [float(v) for v in elems[-wv_dim:]]\n",
    "    return emb\n",
    "embedding = build_embedding(wv_file, vocab, wv_dim)\n",
    "log.info('got embedding matrix.')\n",
    "\n",
    "train.to_csv('SQuAD/train.csv', index=False)\n",
    "dev.to_csv('SQuAD/dev.csv', index=False)\n",
    "meta = {\n",
    "    'vocab': vocab,\n",
    "    'embedding': embedding.tolist()\n",
    "}\n",
    "with open('SQuAD/meta.msgpack', 'wb') as f:\n",
    "    msgpack.dump(meta, f)\n",
    "result = {\n",
    "    'trn_question_ids': question_ids[:len(train)],\n",
    "    'dev_question_ids': question_ids[len(train):],\n",
    "    'trn_context_ids': context_ids[:len(train)],\n",
    "    'dev_context_ids': context_ids[len(train):],\n",
    "    'trn_context_features': context_features[:len(train)],\n",
    "    'dev_context_features': context_features[len(train):],\n",
    "    'trn_context_tags': context_tag_ids[:len(train)],\n",
    "    'dev_context_tags': context_tag_ids[len(train):],\n",
    "    'trn_context_ents': context_ent_ids[:len(train)],\n",
    "    'dev_context_ents': context_ent_ids[len(train):],\n",
    "    'trn_context_text': context_text[:len(train)],\n",
    "    'dev_context_text': context_text[len(train):],\n",
    "    'trn_context_spans': context_token_span[:len(train)],\n",
    "    'dev_context_spans': context_token_span[len(train):]\n",
    "}\n",
    "with open('SQuAD/data.msgpack', 'wb') as f:\n",
    "    msgpack.dump(result, f)\n",
    "if args.sample_size:\n",
    "    sample_size = args.sample_size\n",
    "    sample = {\n",
    "        'trn_question_ids': result['trn_question_ids'][:sample_size],\n",
    "        'dev_question_ids': result['dev_question_ids'][:sample_size],\n",
    "        'trn_context_ids': result['trn_context_ids'][:sample_size],\n",
    "        'dev_context_ids': result['dev_context_ids'][:sample_size],\n",
    "        'trn_context_features': result['trn_context_features'][:sample_size],\n",
    "        'dev_context_features': result['dev_context_features'][:sample_size],\n",
    "        'trn_context_tags': result['trn_context_tags'][:sample_size],\n",
    "        'dev_context_tags': result['dev_context_tags'][:sample_size],\n",
    "        'trn_context_ents': result['trn_context_ents'][:sample_size],\n",
    "        'dev_context_ents': result['dev_context_ents'][:sample_size],\n",
    "        'trn_context_text': result['trn_context_text'][:sample_size],\n",
    "        'dev_context_text': result['dev_context_text'][:sample_size],\n",
    "        'trn_context_spans': result['trn_context_spans'][:sample_size],\n",
    "        'dev_context_spans': result['dev_context_spans'][:sample_size]\n",
    "    }\n",
    "    with open('SQuAD/sample.msgpack', 'wb') as f:\n",
    "        msgpack.dump(sample, f)\n",
    "log.info('saved to disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tagger' object has no attribute 'tag_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-dca264bbf5e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tagger' object has no attribute 'tag_names'"
     ]
    }
   ],
   "source": [
    "print(nlp.tagger.tag_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
