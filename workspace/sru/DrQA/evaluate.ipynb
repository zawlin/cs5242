{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 937\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import string\n",
    "import logging\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import torch\n",
    "import msgpack\n",
    "import pandas as pd\n",
    "from drqa.model import DocReaderModel\n",
    "from drqa.utils import str2bool\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Train a Document Reader model.'\n",
    ")\n",
    "# system\n",
    "parser.add_argument('--log_file', default='output.log',\n",
    "                    help='path for log file.')\n",
    "parser.add_argument('--log_per_updates', type=int, default=3,\n",
    "                    help='log model loss per x updates (mini-batches).')\n",
    "parser.add_argument('--data_file', default='SQuAD/data.msgpack',\n",
    "                    help='path to preprocessed data file.')\n",
    "parser.add_argument('--model_dir', default='models',\n",
    "                    help='path to store saved models.')\n",
    "parser.add_argument('--save_last_only', action='store_true',\n",
    "                    help='only save the final models.')\n",
    "parser.add_argument('--eval_per_epoch', type=int, default=1,\n",
    "                    help='perform evaluation per x epochs.')\n",
    "parser.add_argument('--seed', type=int, default=937,\n",
    "                    help='random seed for data shuffling, dropout, etc.')\n",
    "parser.add_argument(\"--cuda\", type=str2bool, nargs='?',\n",
    "                    const=True, default=torch.cuda.is_available(),\n",
    "                    help='whether to use GPU acceleration.')\n",
    "# training\n",
    "parser.add_argument('-e', '--epochs', type=int, default=50)\n",
    "parser.add_argument('-bs', '--batch_size', type=int, default=32)\n",
    "parser.add_argument('-rs', '--resume', default='',\n",
    "                    help='previous model file name (in `model_dir`). '\n",
    "                         'e.g. \"checkpoint_epoch_11.pt\"')\n",
    "parser.add_argument('-ro', '--resume_options', action='store_true',\n",
    "                    help='use previous model options, ignore the cli and defaults.')\n",
    "parser.add_argument('-rlr', '--reduce_lr', type=float, default=0.,\n",
    "                    help='reduce initial (resumed) learning rate by this factor.')\n",
    "parser.add_argument('-op', '--optimizer', default='adamax',\n",
    "                    help='supported optimizer: adamax, sgd')\n",
    "parser.add_argument('-gc', '--grad_clipping', type=float, default=20)\n",
    "parser.add_argument('-wd', '--weight_decay', type=float, default=0)\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=0.001,\n",
    "                    help='only applied to SGD.')\n",
    "parser.add_argument('-mm', '--momentum', type=float, default=0,\n",
    "                    help='only applied to SGD.')\n",
    "parser.add_argument('-tp', '--tune_partial', type=int, default=1000,\n",
    "                    help='finetune top-x embeddings.')\n",
    "parser.add_argument('--fix_embeddings', action='store_true',\n",
    "                    help='if true, `tune_partial` will be ignored.')\n",
    "parser.add_argument('--rnn_padding', action='store_true',\n",
    "                    help='perform rnn padding (much slower but more accurate).')\n",
    "# model\n",
    "parser.add_argument('--question_merge', default='self_attn')\n",
    "parser.add_argument('--doc_layers', type=int, default=5)\n",
    "parser.add_argument('--question_layers', type=int, default=5)\n",
    "parser.add_argument('--hidden_size', type=int, default=256)\n",
    "parser.add_argument('--num_features', type=int, default=4)\n",
    "parser.add_argument('--pos', type=str2bool, nargs='?', const=True, default=True,\n",
    "                    help='use pos tags as a feature.')\n",
    "parser.add_argument('--pos_size', type=int, default=56,\n",
    "                    help='how many kinds of POS tags.')\n",
    "parser.add_argument('--pos_dim', type=int, default=56,\n",
    "                    help='the embedding dimension for POS tags.')\n",
    "parser.add_argument('--ner', type=str2bool, nargs='?', const=True, default=True,\n",
    "                    help='use named entity tags as a feature.')\n",
    "parser.add_argument('--ner_size', type=int, default=19,\n",
    "                    help='how many kinds of named entity tags.')\n",
    "parser.add_argument('--ner_dim', type=int, default=19,\n",
    "                    help='the embedding dimension for named entity tags.')\n",
    "parser.add_argument('--use_qemb', type=str2bool, nargs='?', const=True, default=True)\n",
    "parser.add_argument('--concat_rnn_layers', type=str2bool, nargs='?',\n",
    "                    const=True, default=False)\n",
    "parser.add_argument('--dropout_emb', type=float, default=0.5)\n",
    "parser.add_argument('--dropout_rnn', type=float, default=0.2)\n",
    "parser.add_argument('--dropout_rnn_output', type=str2bool, nargs='?',\n",
    "                    const=True, default=True)\n",
    "parser.add_argument('--max_len', type=int, default=15)\n",
    "parser.add_argument('--rnn_type', default='lstm',\n",
    "                    help='supported types: rnn, gru, lstm')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# set model dir\n",
    "model_dir = args.model_dir\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_dir = os.path.abspath(model_dir)\n",
    "\n",
    "# set random seed\n",
    "seed = args.seed if args.seed >= 0 else int(random.random()*1000)\n",
    "print ('seed:', seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# setup logger\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(args.log_file)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "ch.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(fmt='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "log.addHandler(fh)\n",
    "log.addHandler(ch)\n",
    "\n",
    "args.resume = 'best_model.pt'\n",
    "def setup():\n",
    "    log.info('[program starts.]')\n",
    "    train, dev, dev_y, embedding, opt = load_data(vars(args))\n",
    "    log.info('[Data loaded.]')\n",
    "\n",
    "    if args.resume:\n",
    "        log.info('[loading previous model...]')\n",
    "        checkpoint = torch.load(os.path.join(model_dir, args.resume))\n",
    "        if args.resume_options:\n",
    "            opt = checkpoint['config']\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        model = DocReaderModel(opt, embedding, state_dict)\n",
    "        epoch_0 = checkpoint['epoch'] + 1\n",
    "        for i in range(checkpoint['epoch']):\n",
    "            random.shuffle(list(range(len(train))))  # synchronize random seed\n",
    "        if args.reduce_lr:\n",
    "            lr_decay(model.optimizer, lr_decay=args.reduce_lr)\n",
    "    else:\n",
    "        model = DocReaderModel(opt, embedding)\n",
    "        epoch_0 = 1\n",
    "\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "def lr_decay(optimizer, lr_decay):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "    log.info('[learning rate reduced by {}]'.format(lr_decay))\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def load_data(opt):\n",
    "    with open('SQuAD/meta.msgpack', 'rb') as f:\n",
    "        meta = msgpack.load(f, encoding='utf8')\n",
    "    embedding = torch.Tensor(meta['embedding'])\n",
    "    opt['pretrained_words'] = True\n",
    "    opt['vocab_size'] = embedding.size(0)\n",
    "    opt['embedding_dim'] = embedding.size(1)\n",
    "    if not opt['fix_embeddings']:\n",
    "        embedding[1] = torch.normal(means=torch.zeros(opt['embedding_dim']), std=1.)\n",
    "    with open(args.data_file, 'rb') as f:\n",
    "        data = msgpack.load(f, encoding='utf8')\n",
    "    train_orig = pd.read_csv('SQuAD/train.csv')\n",
    "    dev_orig = pd.read_csv('SQuAD/dev.csv')\n",
    "    train = list(zip(\n",
    "        data['trn_context_ids'],\n",
    "        data['trn_context_features'],\n",
    "        data['trn_context_tags'],\n",
    "        data['trn_context_ents'],\n",
    "        data['trn_question_ids'],\n",
    "        train_orig['answer_start_token'].tolist(),\n",
    "        train_orig['answer_end_token'].tolist(),\n",
    "        data['trn_context_text'],\n",
    "        data['trn_context_spans']\n",
    "    ))\n",
    "    dev = list(zip(\n",
    "        data['dev_context_ids'],\n",
    "        data['dev_context_features'],\n",
    "        data['dev_context_tags'],\n",
    "        data['dev_context_ents'],\n",
    "        data['dev_question_ids'],\n",
    "        data['dev_context_text'],\n",
    "        data['dev_context_spans']\n",
    "    ))\n",
    "    dev_y = dev_orig['answers'].tolist()[:len(dev)]\n",
    "    dev_y = [eval(y) for y in dev_y]\n",
    "    return train, dev, dev_y, embedding, opt\n",
    "\n",
    "\n",
    "class BatchGen:\n",
    "    def __init__(self, data, batch_size, gpu, evaluation=False):\n",
    "        '''\n",
    "        input:\n",
    "            data - list of lists\n",
    "            batch_size - int\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.eval = evaluation\n",
    "        self.gpu = gpu\n",
    "\n",
    "        # shuffle\n",
    "        if not evaluation:\n",
    "            indices = list(range(len(data)))\n",
    "            random.shuffle(indices)\n",
    "            data = [data[i] for i in indices]\n",
    "        # chunk into batches\n",
    "        data = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.data:\n",
    "            batch_size = len(batch)\n",
    "            batch = list(zip(*batch))\n",
    "            if self.eval:\n",
    "                assert len(batch) == 7\n",
    "            else:\n",
    "                assert len(batch) == 9\n",
    "\n",
    "            context_len = max(len(x) for x in batch[0])\n",
    "            context_id = torch.LongTensor(batch_size, context_len).fill_(0)\n",
    "            for i, doc in enumerate(batch[0]):\n",
    "                context_id[i, :len(doc)] = torch.LongTensor(doc)\n",
    "\n",
    "            feature_len = len(batch[1][0][0])\n",
    "            context_feature = torch.Tensor(batch_size, context_len, feature_len).fill_(0)\n",
    "            for i, doc in enumerate(batch[1]):\n",
    "                for j, feature in enumerate(doc):\n",
    "                    context_feature[i, j, :] = torch.Tensor(feature)\n",
    "\n",
    "            context_tag = torch.LongTensor(batch_size, context_len).fill_(0)\n",
    "            for i, doc in enumerate(batch[2]):\n",
    "                context_tag[i, :len(doc)] = torch.LongTensor(doc)\n",
    "\n",
    "            context_ent = torch.LongTensor(batch_size, context_len).fill_(0)\n",
    "            for i, doc in enumerate(batch[3]):\n",
    "                context_ent[i, :len(doc)] = torch.LongTensor(doc)\n",
    "            question_len = max(len(x) for x in batch[4])\n",
    "            question_id = torch.LongTensor(batch_size, question_len).fill_(0)\n",
    "            for i, doc in enumerate(batch[4]):\n",
    "                question_id[i, :len(doc)] = torch.LongTensor(doc)\n",
    "\n",
    "            context_mask = torch.eq(context_id, 0)\n",
    "            question_mask = torch.eq(question_id, 0)\n",
    "            if not self.eval:\n",
    "                y_s = torch.LongTensor(batch[5])\n",
    "                y_e = torch.LongTensor(batch[6])\n",
    "            text = list(batch[-2])\n",
    "            span = list(batch[-1])\n",
    "            if self.gpu:\n",
    "                context_id = context_id.pin_memory()\n",
    "                context_feature = context_feature.pin_memory()\n",
    "                context_tag = context_tag.pin_memory()\n",
    "                context_ent = context_ent.pin_memory()\n",
    "                context_mask = context_mask.pin_memory()\n",
    "                question_id = question_id.pin_memory()\n",
    "                question_mask = question_mask.pin_memory()\n",
    "            if self.eval:\n",
    "                yield (context_id, context_feature, context_tag, context_ent, context_mask,\n",
    "                       question_id, question_mask, text, span)\n",
    "            else:\n",
    "                yield (context_id, context_feature, context_tag, context_ent, context_mask,\n",
    "                       question_id, question_mask, y_s, y_e, text, span)\n",
    "\n",
    "\n",
    "def _normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def _exact_match(pred, answers):\n",
    "    if pred is None or answers is None:\n",
    "        return False\n",
    "    pred = _normalize_answer(pred)\n",
    "    for a in answers:\n",
    "        if pred == _normalize_answer(a):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _f1_score(pred, answers):\n",
    "    def _score(g_tokens, a_tokens):\n",
    "        common = Counter(g_tokens) & Counter(a_tokens)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            return 0\n",
    "        precision = 1. * num_same / len(g_tokens)\n",
    "        recall = 1. * num_same / len(a_tokens)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "\n",
    "    if pred is None or answers is None:\n",
    "        return 0\n",
    "    g_tokens = _normalize_answer(pred).split()\n",
    "    scores = [_score(g_tokens, _normalize_answer(a).split()) for a in answers]\n",
    "    return max(scores)\n",
    "\n",
    "\n",
    "def score(pred, truth):\n",
    "    assert len(pred) == len(truth)\n",
    "    f1 = em = total = 0\n",
    "    for p, t in zip(pred, truth):\n",
    "        total += 1\n",
    "        em += _exact_match(p, t)\n",
    "        f1 += _f1_score(p, t)\n",
    "    em = 100. * em / total\n",
    "    f1 = 100. * f1 / total\n",
    "    return em, f1\n",
    "\n",
    "#setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10/2017 05:08:19 [program starts.]\n",
      "11/10/2017 05:08:19 [program starts.]\n",
      "11/10/2017 05:08:45 [Data loaded.]\n",
      "11/10/2017 05:08:45 [Data loaded.]\n",
      "11/10/2017 05:08:45 [loading previous model...]\n",
      "11/10/2017 05:08:45 [loading previous model...]\n",
      "8926310 parameters\n"
     ]
    }
   ],
   "source": [
    "log.info('[program starts.]')\n",
    "train, dev, dev_y, embedding, opt = load_data(vars(args))\n",
    "log.info('[Data loaded.]')\n",
    "\n",
    "\n",
    "log.info('[loading previous model...]')\n",
    "checkpoint = torch.load('/home/zawlin/g/sru/DrQA2/models_bs128_h256/best_model.pt')\n",
    "\n",
    "if args.resume_options:\n",
    "    opt = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = DocReaderModel(opt, embedding, state_dict)\n",
    "epoch_0 = checkpoint['epoch'] + 1\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10/2017 05:09:37 [dev EM: 60.47295460723023 F1: 73.65263234670812]\n",
      "11/10/2017 05:09:37 [dev EM: 60.47295460723023 F1: 73.65263234670812]\n"
     ]
    }
   ],
   "source": [
    "batches = BatchGen(dev, batch_size=64, evaluation=True, gpu=args.cuda)\n",
    "predictions = []\n",
    "for batch in batches:\n",
    "    predictions.extend(model.predict(batch))\n",
    "em, f1 = score(predictions, dev_y)\n",
    "log.info(\"[dev EM: {} F1: {}]\".format(em, f1))\n",
    "best_val_score = em\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f6c40e2fa638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mans_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mans_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "dev_orig = pd.read_csv('SQuAD/dev.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(answer_single.keys())\n",
    "import string\n",
    "import re\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "ret='Id,Answer\\r\\n'\n",
    "for i in range(len(predictions)):\n",
    "    ret += str(dev_orig['id'][i]) + ','+normalize_answer(predictions[i])+'\\r\\n'\n",
    "    \n",
    "f=open('/home/zawlin/answers/5.csv','w')\n",
    "f.write(ret)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
