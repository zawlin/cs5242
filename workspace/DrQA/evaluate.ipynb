{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import string\n",
    "import logging\n",
    "import argparse\n",
    "from shutil import copyfile\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import torch\n",
    "import msgpack\n",
    "from drqa.model import DocReaderModel\n",
    "from drqa.utils import str2bool\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Train a Document Reader model.'\n",
    ")\n",
    "# system\n",
    "parser.add_argument('--log_file', default='output.log',\n",
    "                    help='path for log file.')\n",
    "parser.add_argument('--log_per_updates', type=int, default=3,\n",
    "                    help='log model loss per x updates (mini-batches).')\n",
    "parser.add_argument('--data_file', default='SQuAD/data.msgpack',\n",
    "                    help='path to preprocessed data file.')\n",
    "parser.add_argument('--model_dir', default='models',\n",
    "                    help='path to store saved models.')\n",
    "parser.add_argument('--save_last_only', action='store_true',\n",
    "                    help='only save the final models.')\n",
    "parser.add_argument('--eval_per_epoch', type=int, default=1,\n",
    "                    help='perform evaluation per x epochs.')\n",
    "parser.add_argument('--seed', type=int, default=1013,\n",
    "                    help='random seed for data shuffling, dropout, etc.')\n",
    "parser.add_argument(\"--cuda\", type=str2bool, nargs='?',\n",
    "                    const=True, default=torch.cuda.is_available(),\n",
    "                    help='whether to use GPU acceleration.')\n",
    "# training\n",
    "parser.add_argument('-e', '--epochs', type=int, default=40)\n",
    "parser.add_argument('-bs', '--batch_size', type=int, default=32)\n",
    "parser.add_argument('-rs', '--resume', default='',\n",
    "                    help='previous model file name (in `model_dir`). '\n",
    "                         'e.g. \"checkpoint_epoch_11.pt\"')\n",
    "parser.add_argument('-ro', '--resume_options', action='store_true',\n",
    "                    help='use previous model options, ignore the cli and defaults.')\n",
    "parser.add_argument('-rlr', '--reduce_lr', type=float, default=0.,\n",
    "                    help='reduce initial (resumed) learning rate by this factor.')\n",
    "parser.add_argument('-op', '--optimizer', default='adamax',\n",
    "                    help='supported optimizer: adamax, sgd')\n",
    "parser.add_argument('-gc', '--grad_clipping', type=float, default=10)\n",
    "parser.add_argument('-wd', '--weight_decay', type=float, default=0)\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=0.1,\n",
    "                    help='only applied to SGD.')\n",
    "parser.add_argument('-mm', '--momentum', type=float, default=0,\n",
    "                    help='only applied to SGD.')\n",
    "parser.add_argument('-tp', '--tune_partial', type=int, default=1000,\n",
    "                    help='finetune top-x embeddings.')\n",
    "parser.add_argument('--fix_embeddings', action='store_true',\n",
    "                    help='if true, `tune_partial` will be ignored.')\n",
    "parser.add_argument('--rnn_padding', action='store_true',\n",
    "                    help='perform rnn padding (much slower but more accurate).')\n",
    "# model\n",
    "parser.add_argument('--question_merge', default='self_attn')\n",
    "parser.add_argument('--doc_layers', type=int, default=3)\n",
    "parser.add_argument('--question_layers', type=int, default=3)\n",
    "parser.add_argument('--hidden_size', type=int, default=128)\n",
    "parser.add_argument('--num_features', type=int, default=4)\n",
    "parser.add_argument('--pos', type=str2bool, nargs='?', const=True, default=True,\n",
    "                    help='use pos tags as a feature.')\n",
    "parser.add_argument('--ner', type=str2bool, nargs='?', const=True, default=True,\n",
    "                    help='use named entity tags as a feature.')\n",
    "parser.add_argument('--use_qemb', type=str2bool, nargs='?', const=True, default=True)\n",
    "parser.add_argument('--concat_rnn_layers', type=str2bool, nargs='?',\n",
    "                    const=True, default=True)\n",
    "parser.add_argument('--dropout_emb', type=float, default=0.4)\n",
    "parser.add_argument('--dropout_rnn', type=float, default=0.4)\n",
    "parser.add_argument('--dropout_rnn_output', type=str2bool, nargs='?',\n",
    "                    const=True, default=True)\n",
    "parser.add_argument('--max_len', type=int, default=15)\n",
    "parser.add_argument('--rnn_type', default='lstm',\n",
    "                    help='supported types: rnn, gru, lstm')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# set model dir\n",
    "model_dir = args.model_dir\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_dir = os.path.abspath(model_dir)\n",
    "\n",
    "# set random seed\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# setup logger\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(args.log_file)\n",
    "fh.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "ch.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(fmt='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "log.addHandler(fh)\n",
    "log.addHandler(ch)\n",
    "\n",
    "\n",
    "def main():\n",
    "    log.info('[program starts.]')\n",
    "    train, dev, dev_y, embedding, opt = load_data(vars(args))\n",
    "    log.info(opt)\n",
    "    log.info('[Data loaded.]')\n",
    "\n",
    "    if args.resume:\n",
    "        log.info('[loading previous model...]')\n",
    "        checkpoint = torch.load(os.path.join(model_dir, args.resume))\n",
    "        if args.resume_options:\n",
    "            opt = checkpoint['config']\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        model = DocReaderModel(opt, embedding, state_dict)\n",
    "        epoch_0 = checkpoint['epoch'] + 1\n",
    "        for i in range(checkpoint['epoch']):\n",
    "            random.shuffle(list(range(len(train))))  # synchronize random seed\n",
    "        if args.reduce_lr:\n",
    "            lr_decay(model.optimizer, lr_decay=args.reduce_lr)\n",
    "    else:\n",
    "        model = DocReaderModel(opt, embedding)\n",
    "        epoch_0 = 1\n",
    "\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    if args.resume:\n",
    "        batches = BatchGen(dev, batch_size=args.batch_size, evaluation=True, gpu=args.cuda)\n",
    "        predictions = []\n",
    "        for batch in batches:\n",
    "            predictions.extend(model.predict(batch))\n",
    "        em, f1 = score(predictions, dev_y)\n",
    "        log.info(\"[dev EM: {} F1: {}]\".format(em, f1))\n",
    "        best_val_score = em\n",
    "    else:\n",
    "        best_val_score = 0.0\n",
    "\n",
    "    for epoch in range(epoch_0, epoch_0 + args.epochs):\n",
    "        log.warning('Epoch {}'.format(epoch))\n",
    "        # train\n",
    "        batches = BatchGen(train, batch_size=args.batch_size, gpu=args.cuda)\n",
    "        start = datetime.now()\n",
    "        for i, batch in enumerate(batches):\n",
    "            model.update(batch)\n",
    "            if i % args.log_per_updates == 0:\n",
    "                log.info('epoch [{0:2}] updates[{1:6}] train loss[{2:.5f}] remaining[{3}]'.format(\n",
    "                    epoch, model.updates, model.train_loss.avg,\n",
    "                    str((datetime.now() - start) / (i + 1) * (len(batches) - i - 1)).split('.')[0]))\n",
    "        # eval\n",
    "        if epoch % args.eval_per_epoch == 0:\n",
    "            batches = BatchGen(dev, batch_size=args.batch_size, evaluation=True, gpu=args.cuda)\n",
    "            predictions = []\n",
    "            for batch in batches:\n",
    "                predictions.extend(model.predict(batch))\n",
    "            em, f1 = score(predictions, dev_y)\n",
    "            log.warning(\"dev EM: {} F1: {}\".format(em, f1))\n",
    "        # save\n",
    "        if not args.save_last_only or epoch == epoch_0 + args.epochs - 1:\n",
    "            model_file = os.path.join(model_dir, 'checkpoint_epoch_{}.pt'.format(epoch))\n",
    "            model.save(model_file, epoch)\n",
    "            if em > best_val_score:\n",
    "                best_val_score = em\n",
    "                copyfile(\n",
    "                    model_file,\n",
    "                    os.path.join(model_dir, 'best_model.pt'))\n",
    "                log.info('[neGw best model saved.]')\n",
    "\n",
    "\n",
    "def lr_decay(optimizer, lr_decay):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "    log.info('[learning rate reduced by {}]'.format(lr_decay))\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def load_data(opt):\n",
    "    with open('SQuAD/meta.msgpack', 'rb') as f:\n",
    "        meta = msgpack.load(f, encoding='utf8')\n",
    "    embedding = torch.Tensor(meta['embedding'])\n",
    "    opt['pretrained_words'] = True\n",
    "    opt['vocab_size'] = embedding.size(0)\n",
    "    opt['embedding_dim'] = embedding.size(1)\n",
    "    opt['pos_size'] = len(meta['vocab_tag'])\n",
    "    opt['ner_size'] = len(meta['vocab_ent'])\n",
    "    with open(args.data_file, 'rb') as f:\n",
    "        data = msgpack.load(f, encoding='utf8')\n",
    "    train = data['train']\n",
    "    data['dev'].sort(key=lambda x: len(x[1]))\n",
    "    dev = [x[:-1] for x in data['dev']]\n",
    "    dev_y = [x[-1] for x in data['dev']]\n",
    "    return train, dev, dev_y, embedding, opt\n",
    "\n",
    "\n",
    "class BatchGen:\n",
    "    def __init__(self, data, batch_size, gpu, evaluation=False):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            data - list of lists\n",
    "            batch_size - int\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.eval = evaluation\n",
    "        self.gpu = gpu\n",
    "\n",
    "        # shuffle\n",
    "        if not evaluation:\n",
    "            indices = list(range(len(data)))\n",
    "            random.shuffle(indices)\n",
    "            data = [data[i] for i in indices]\n",
    "        # chunk into batches\n",
    "        data = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.data:\n",
    "            batch_size = len(batch)\n",
    "            batch = list(zip(*batch))\n",
    "            if self.eval:\n",
    "                assert len(batch) == 8\n",
    "            else:\n",
    "                assert len(batch) == 10\n",
    "\n",
    "            context_len = max(len(x) for x in batch[1])\n",
    "            context_id = torch.LongTensor(batch_size, context_len).fill_(0)\n",
    "            for i, doc in enumerate(batch[1]):\n",
    "                context_id[i, :len(doc)] = torch.LongTensor(doc)\n",
    "\n",
    "            feature_len = len(batch[2][0][0])\n",
    "\n",
    "            context_feature = torch.Tensor(batch_size, context_len, feature_len).fill_(0)\n",
    "            for i, doc in enumerate(batch[2]):\n",
    "                for j, feature in enumerate(doc):\n",
    "                    context_feature[i, j, :] = torch.Tensor(feature)\n",
    "\n",
    "            context_tag = torch.Tensor(batch_size, context_len, args.pos_size).fill_(0)\n",
    "            for i, doc in enumerate(batch[3]):\n",
    "                for j, tag in enumerate(doc):\n",
    "                    context_tag[i, j, tag] = 1\n",
    "\n",
    "            context_ent = torch.Tensor(batch_size, context_len, args.ner_size).fill_(0)\n",
    "            for i, doc in enumerate(batch[4]):\n",
    "                for j, ent in enumerate(doc):\n",
    "                    context_ent[i, j, ent] = 1\n",
    "\n",
    "            question_len = max(len(x) for x in batch[5])\n",
    "            question_id = torch.LongTensor(batch_size, question_len).fill_(0)\n",
    "            for i, doc in enumerate(batch[5]):\n",
    "                question_id[i, :len(doc)] = torch.LongTensor(doc)\n",
    "\n",
    "            context_mask = torch.eq(context_id, 0)\n",
    "            question_mask = torch.eq(question_id, 0)\n",
    "            text = list(batch[6])\n",
    "            span = list(batch[7])\n",
    "            if not self.eval:\n",
    "                y_s = torch.LongTensor(batch[8])\n",
    "                y_e = torch.LongTensor(batch[9])\n",
    "            if self.gpu:\n",
    "                context_id = context_id.pin_memory()\n",
    "                context_feature = context_feature.pin_memory()\n",
    "                context_tag = context_tag.pin_memory()\n",
    "                context_ent = context_ent.pin_memory()\n",
    "                context_mask = context_mask.pin_memory()\n",
    "                question_id = question_id.pin_memory()\n",
    "                question_mask = question_mask.pin_memory()\n",
    "            if self.eval:\n",
    "                yield (context_id, context_feature, context_tag, context_ent, context_mask,\n",
    "                       question_id, question_mask, text, span)\n",
    "            else:\n",
    "                yield (context_id, context_feature, context_tag, context_ent, context_mask,\n",
    "                       question_id, question_mask, y_s, y_e, text, span)\n",
    "\n",
    "\n",
    "def _normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def _exact_match(pred, answers):\n",
    "    if pred is None or answers is None:\n",
    "        return False\n",
    "    pred = _normalize_answer(pred)\n",
    "    for a in answers:\n",
    "        if pred == _normalize_answer(a):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _f1_score(pred, answers):\n",
    "    def _score(g_tokens, a_tokens):\n",
    "        common = Counter(g_tokens) & Counter(a_tokens)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            return 0\n",
    "        precision = 1. * num_same / len(g_tokens)\n",
    "        recall = 1. * num_same / len(a_tokens)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "\n",
    "    if pred is None or answers is None:\n",
    "        return 0\n",
    "    g_tokens = _normalize_answer(pred).split()\n",
    "    scores = [_score(g_tokens, _normalize_answer(a).split()) for a in answers]\n",
    "    return max(scores)\n",
    "\n",
    "\n",
    "def score(pred, truth):\n",
    "    assert len(pred) == len(truth)\n",
    "    f1 = em = total = 0\n",
    "    for p, t in zip(pred, truth):\n",
    "        total += 1\n",
    "        em += _exact_match(p, t)\n",
    "        f1 += _f1_score(p, t)\n",
    "    em = 100. * em / total\n",
    "    f1 = 100. * f1 / total\n",
    "    return em, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10/2017 03:15:51 [program starts.]\n",
      "11/10/2017 03:16:22 {'dropout_emb': 0.4, 'dropout_rnn': 0.4, 'grad_clipping': 10, 'pos_size': 51, 'log_file': 'output.log', 'rnn_padding': False, 'vocab_size': 91555, 'weight_decay': 0, 'embedding_dim': 300, 'eval_per_epoch': 1, 'resume': '', 'reduce_lr': 0.0, 'learning_rate': 0.1, 'fix_embeddings': False, 'concat_rnn_layers': True, 'resume_options': False, 'hidden_size': 128, 'save_last_only': False, 'log_per_updates': 3, 'epochs': 40, 'ner_size': 19, 'pos': True, 'batch_size': 32, 'cuda': True, 'question_layers': 3, 'rnn_type': 'lstm', 'pretrained_words': True, 'dropout_rnn_output': True, 'optimizer': 'adamax', 'ner': True, 'use_qemb': True, 'num_features': 4, 'seed': 1013, 'tune_partial': 1000, 'question_merge': 'self_attn', 'max_len': 15, 'momentum': 0, 'doc_layers': 3, 'model_dir': 'models', 'data_file': 'SQuAD/data.msgpack'}\n",
      "11/10/2017 03:16:22 [Data loaded.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log.info('[program starts.]')\n",
    "train, dev, dev_y, embedding, opt = load_data(vars(args))\n",
    "log.info(opt)\n",
    "log.info('[Data loaded.]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load('/home/zawlin/g/DrQA2/models_e80_bs128/best_model.pt')\n",
    "if args.resume_options:\n",
    "    opt = checkpoint['config']\n",
    "state_dict = checkpoint['state_dict']\n",
    "model = DocReaderModel(opt, embedding, state_dict)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-10-e39b283d226f>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-e39b283d226f>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    return train, dev, dev_y, embedding, opt\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('SQuAD/meta.msgpack', 'rb') as f:\n",
    "    meta = msgpack.load(f, encoding='utf8')\n",
    "embedding = torch.Tensor(meta['embedding'])\n",
    "opt['pretrained_words'] = True\n",
    "opt['vocab_size'] = embedding.size(0)\n",
    "opt['embedding_dim'] = embedding.size(1)\n",
    "opt['pos_size'] = len(meta['vocab_tag'])\n",
    "opt['ner_size'] = len(meta['vocab_ent'])\n",
    "with open(args.data_file, 'rb') as f:\n",
    "    data = msgpack.load(f, encoding='utf8')\n",
    "train = data['train']\n",
    "data['dev'].sort(key=lambda x: len(x[1]))\n",
    "dev = [x[:-1] for x in data['dev']]\n",
    "dev_y = [x[-1] for x in data['dev']]\n",
    "#return train, dev, dev_y, embedding, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['067438', [944, 1045, 82, 6439, 821, 1907, 29292, 920, 928, 31, 6, 1668, 18, 51, 35042, 825, 22, 2118, 52621, 7, 6473, 42205, 2454, 216], [[False, False, False, 0.08333333333333333], [True, True, True, 0.041666666666666664], [True, True, True, 0.041666666666666664], [False, False, True, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, True, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.08333333333333333], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664], [False, False, False, 0.041666666666666664]], [4, 4, 0, 5, 17, 4, 5, 21, 4, 1, 1, 0, 8, 4, 5, 19, 24, 17, 10, 18, 10, 4, 5, 7], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 9, 12, 44, 5, 1045, 82, 11907, 2], 'Other immune system disorders include various hypersensitivities (such as in asthma and other allergies) that respond inappropriately to otherwise harmless compounds.', [[0, 5], [6, 12], [13, 19], [20, 29], [30, 37], [38, 45], [46, 64], [65, 66], [66, 70], [71, 73], [74, 76], [77, 83], [84, 87], [88, 93], [94, 103], [103, 104], [105, 109], [110, 117], [118, 133], [134, 136], [137, 146], [147, 155], [156, 165], [165, 166]], ['hypersensitivities']]\n"
     ]
    }
   ],
   "source": [
    "print(data['dev'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10/2017 05:15:23 [dev EM: 59.5080184832835 F1: 72.54025201624347]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batches = BatchGen(dev, batch_size=args.batch_size, evaluation=True, gpu=args.cuda)\n",
    "predictions = []\n",
    "for batch in batches:\n",
    "    predictions.extend(model.predict(batch))\n",
    "em, f1 = score(predictions, dev_y)\n",
    "log.info(\"[dev EM: {} F1: {}]\".format(em, f1))\n",
    "best_val_score = em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(answer_single.keys())\n",
    "import string\n",
    "import re\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "#ret='Id,Answer\\r\\n'\n",
    "#for i in range(len(predictions)):\n",
    "#    ret += str(data['dev'][i][0]) + ','+normalize_answer(predictions[i])+'\\r\\n'\n",
    "    \n",
    "ret='Id,Answer\\r\\n'\n",
    "for i in range(len(predictions)):\n",
    "    ret += str(data['dev'][i][0]) + ','+normalize_answer(predictions[i])+'\\r\\n'\n",
    "f=open('/home/zawlin/answers/6.csv','w')\n",
    "f.write(ret)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5840717586300626, 0.5939113889643926, 0.5438434357162273, 0.5432182658331068, 0.5933677629790703, 0.5833922261484099]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_ans_dict(aa,path):    \n",
    "    lines = tuple(open(path, 'r'))[1:]\n",
    "    ret = []\n",
    "    for l in lines:\n",
    "        ls = l.split(',')\n",
    "        if ls[0] in aa:\n",
    "            aa[ls[0]].append(ls[1].strip())\n",
    "        else:\n",
    "            aa[ls[0]]=[]\n",
    "            aa[ls[0]].append(ls[1].strip())\n",
    "                \n",
    "def get_ans_dict1(aa,path):    \n",
    "    lines = tuple(open(path, 'r'))[1:]\n",
    "    ret = []\n",
    "    for l in lines:\n",
    "        ls = l.split(',')\n",
    "        assert ls[0] in aa,str(ls[0])\n",
    "        if ls[0] in aa:\n",
    "            aa[ls[0]].append(ls[1].strip())\n",
    "        else:\n",
    "            aa[ls[0]]=[]\n",
    "            aa[ls[0]].append(ls[1].strip())\n",
    "                \n",
    "def fill_gt(aa,path):    \n",
    "    lines = tuple(open(path, 'r'))[1:]\n",
    "    ret = []\n",
    "    for l in lines:\n",
    "        ls = l.split(',')\n",
    "        aa[ls[0]].append(ls[1].strip())\n",
    "ans_all = {}\n",
    "\n",
    "get_ans_dict(ans_all,'/home/zawlin/answers/1.csv')\n",
    "get_ans_dict(ans_all,'/home/zawlin/answers/2.csv')\n",
    "get_ans_dict(ans_all,'/home/zawlin/answers/3.csv')\n",
    "get_ans_dict(ans_all,'/home/zawlin/answers/4.csv')\n",
    "get_ans_dict(ans_all,'/home/zawlin/answers/5.csv')\n",
    "get_ans_dict(ans_all,'/home/zawlin/answers/6.csv')\n",
    "\n",
    "fill_gt(ans_all,'/home/zawlin/answer_gt.csv')\n",
    "w = [0,0,0,0,0,0]\n",
    "\n",
    "def find_auto_weight(w,ans_all):\n",
    "    \n",
    "    for i in range(len(w)):\n",
    "        \n",
    "        total = float(len(ans_all.keys()))\n",
    "        correct = 0.\n",
    "        for a in ans_all.keys():   \n",
    "            ans = ans_all[a]\n",
    "            #print(len(ans))\n",
    "            gt = ans[-1]\n",
    "            \n",
    "            #print(predict)\n",
    "            if gt == ans[i]:\n",
    "                correct += 1\n",
    "                #print(ans)\n",
    "                \n",
    "        w[i]=correct/total\n",
    "find_auto_weight(w,ans_all)\n",
    "print(w)\n",
    "#ans_gt = get_ans_dict('/home/zawlin/answer_gt.csv')\n",
    "#for a in pd.read_csv('/home/zawlin/answers/1.csv'):\n",
    " #   print(a)\n",
    "  #  break\n",
    "    \n",
    "#pd.read_csv('/home/zawlin/answers/2.csv')\n",
    "#pd.read_csv('/home/zawlin/answers/3.csv')\n",
    "\n",
    "#answer_gt = pd.read_csv('/home/zawlin/answer_gt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "22624.0\n",
      "0.6149497145963577\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "total = float(len(ans_all.keys()))\n",
    "correct = 0.\n",
    "dup =0.\n",
    "w[:]=w_orig[:]\n",
    "#w[0]=1.1\n",
    "#w[1]*=.8\n",
    "#w[2]*=1.1\n",
    "#w[4]*=1.2\n",
    "#w[5]*=.9\n",
    "#w[5]*=.8\n",
    "#w[1]*=1.1\n",
    "for a in ans_all.keys():   \n",
    "    ans = ans_all[a]\n",
    "    gt = ans[-1]\n",
    "    voter = {}\n",
    "    #print(ans)\n",
    "    for i in range(len(w)):\n",
    "        if ans[i] in voter:\n",
    "            voter[ans[i]] +=  w[i]\n",
    "        else:\n",
    "            voter[ans[i]] = w[i]\n",
    "    sorted_x = sorted(voter.items(), key=operator.itemgetter(1))[::-1]\n",
    "    \n",
    "    predict = sorted_x[0][0]\n",
    "    \n",
    "    if gt == predict:\n",
    "        correct += 1\n",
    "    else:\n",
    "        pass\n",
    "        #print(ans)\n",
    "print(dup/total)\n",
    "print(correct)\n",
    "print(correct/total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "total = float(len(ans_all.keys()))\n",
    "correct = 0.\n",
    "dup =0.\n",
    "ret = 'Id,Answer\\r\\n'\n",
    "for a in ans_all.keys():   \n",
    "    ans = ans_all[a]\n",
    "    voter = {}\n",
    "    for i in range(len(w)):\n",
    "        if ans[i] in voter:\n",
    "            voter[ans[i]] +=  w[i]\n",
    "        else:\n",
    "            voter[ans[i]] = w[i]\n",
    "    sorted_x = sorted(voter.items(), key=operator.itemgetter(1))[::-1]\n",
    "    \n",
    "    predict = sorted_x[0][0]\n",
    "    \n",
    "    ret+=a + ',' +predict+'\\r\\n'\n",
    "\n",
    "f=open('/home/zawlin/answer_f2.csv','w')\n",
    "f.write(ret)\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5840717586300626, 0.5939113889643926, 0.5438434357162273, 0.5432182658331068, 0.5933677629790703, 0.5833922261484099]\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
